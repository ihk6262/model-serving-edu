version: '2.3'

services:

  # tf2svg1:    # tf serving CPU, without GPU
  #   #build:
  #   #  context: ./covid19/covid19_models
  #   image: tensorflow/serving
  #   hostname: tf2svg1
  #   environment:
  #     - MODEL_NAME=covid19
  #   restart: on-failure
  #   ports:
  #     - 8511:8501 # tensorflow servring port
  #     - 8510:8500 # grpc port
  #   volumes:
  #     - ./covid19/head-base-covid:/models/covid19

  # flask:    # flask server for covid19 service web pages and APIs, with backend tensorflow servering
  #   build:
  #     context: ./covid19
  #   image: ai-service-flask:0.1.0
  #   hostname: flask
  #   restart: on-failure
  #   ports:
  #     - 8051:5000 # tensorflow servring port (위부로는 5000 에서, 내부로는 8051~3 까지 3개를 쓸거라고 정의 인스턴스 수에따라 변경하면됨.)
  #   volumes:
  #     - ./covid19:/app  # tf2 serving convention (covid19 폴더를 app 으로 마운트 하겠다.)

  fastapi:    # FastAPI with Uvicorn server for covid19 service web pages and APIs, with backend tensorflow servering
    build:
      context: ./covid-fastapi
    image: ai-service-fastapi:0.2.0
    hostname: fastapi
    restart: on-failure
    ports:
      - 8051:8000 # tensorflow servring port
    volumes:
      - ./covid-fastapi:/app  # tf2 serving convention

  fastapi-2:    # FastAPI with Uvicorn server for covid19 service web pages and APIs, with backend tensorflow servering
    build:
      context: ./covid-fastapi
    image: ai-service-fastapi:0.2.0
    hostname: fastapi-2
    restart: on-failure
    ports:
      - 8052:8000 # tensorflow servring port
    volumes:
      - ./covid-fastapi:/app  # tf2 serving convention

  lb:
    build:
      context: ./haproxy
    image: 'haproxy:0.0.1'
    volumes:
      - ./haproxy:/haproxy-override
    links:
      - fastapi
      - fastapi-2
    ports:
      - 8088:8088
    expose:
      - 8088
